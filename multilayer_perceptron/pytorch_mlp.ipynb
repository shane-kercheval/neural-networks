{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from data import get_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_available_device() -> str:\n",
    "    \"\"\"Returns the device based on the system configuration.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda').type\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device('mps').type  # https://pytorch.org/docs/stable/notes/mps.html\n",
    "    return torch.device('cpu').type\n",
    "\n",
    "device = get_available_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
     ]
    }
   ],
   "source": [
    "words = get_names()\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "27\n",
      "{'.': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "characters = ['.', *sorted({c for w in words for c in w})]\n",
    "print(characters)\n",
    "print(len(characters))\n",
    "index = {\n",
    "    c:i\n",
    "    for i, c in enumerate(characters)\n",
    "}\n",
    "index\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  5 13 13  1  0 15 12  9 22]\n",
      "[ 5 13 13  1  0 15 12  9 22  9]\n"
     ]
    }
   ],
   "source": [
    "# create the training set of all of the bigrams\n",
    "# x is the input, y is the target (i.e. the next character)\n",
    "x, y = [], []\n",
    "training_data = []\n",
    "for w in words:\n",
    "    chars = ['.',  *w, '.']\n",
    "    for c1, c2 in zip(chars, chars[1:]):\n",
    "        x.append(index[c1])\n",
    "        y.append(index[c2])\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "print(x[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xffff2a2f1a10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAADjCAYAAADZh11QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAATFElEQVR4nO3df2zU9eHH8Vdb6FH1elKwP05KKSiitMDGj8rIGAsNhSkBJQsoS2pn2NRDhEbUmkAlzp0/FtLMEdxMpvxBEU3WoWaDkGohhCLYDpVkK9AZqZbSafQOyjxK7/39w6/3/Z5Q6rXv3qd3PB/JJ2k/9/nxyjtv4MXn87m7FGOMEQAAgAWpTgcAAADJg2IBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwJph8T5hOBxWe3u73G63UlJS4n16AADQD8YYnT17Vl6vV6mpvV+XiHuxaG9vV35+frxPCwAALGhra9OYMWN6fT3uxcLtdkuSPmkep8zrBnYn5q6JxTYiAQCAPlxUtw7ob5F/x3sT92Lx7e2PzOtSlekeWLEYljLcRiQAANCX//1msb4eY+DhTQAAYA3FAgAAWNOvYrFlyxaNGzdOI0aMUElJiQ4fPmw7FwAASEAxF4udO3eqsrJS1dXVam5u1tSpU1VWVqbOzs7ByAcAABJIzMVi8+bNWrVqlSoqKnTbbbfppZde0jXXXKM///nPg5EPAAAkkJiKxYULF9TU1KTS0tL/O0BqqkpLS9XY2HjZfUKhkILBYNQCAACSU0zF4vPPP1dPT49ycnKi1ufk5Kijo+Oy+/j9fnk8nsjCh2MBAJC8Bv1dIVVVVQoEApGlra1tsE8JAAAcEtMHZI0ePVppaWk6c+ZM1PozZ84oNzf3svu4XC65XK7+JwQAAAkjpisW6enpmj59uurr6yPrwuGw6uvrNXv2bOvhAABAYon5I70rKytVXl6uGTNmaNasWaqpqVFXV5cqKioGIx8AAEggMReL5cuX6z//+Y82btyojo4OTZs2Tbt3777kgU4AAHD1STHGmHieMBgMyuPx6Mvj4wf8JWRl3ml2QgEAgCu6aLrVoF0KBALKzMzsdTu+KwQAAFgT969N/9ZdE4v52vM42dN+1MpxuEIEAOgLVywAAIA1FAsAAGANxQIAAFhDsQAAANZQLAAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgDcUCAABYQ7EAAADWUCwAAIA1FAsAAGANxQIAAFhDsQAAANZQLAAAgDUUCwAAYM0wpwNg8JV5pzkdAUliT/tRK8dhTgLJiysWAADAGooFAACwhmIBAACsoVgAAABrYioWfr9fM2fOlNvtVnZ2tpYuXaqWlpbBygYAABJMTMVi37598vl8OnTokPbu3avu7m4tWLBAXV1dg5UPAAAkkJjebrp79+6o31999VVlZ2erqalJc+fOtRoMAAAkngF9jkUgEJAkZWVl9bpNKBRSKBSK/B4MBgdySgAAMIT1++HNcDistWvXas6cOSoqKup1O7/fL4/HE1ny8/P7e0oAADDE9btY+Hw+HTt2TK+99toVt6uqqlIgEIgsbW1t/T0lAAAY4vp1K2T16tV6++23tX//fo0ZM+aK27pcLrlcrn6FAwAAiSWmYmGM0cMPP6y6ujo1NDSosLBwsHIBAIAEFFOx8Pl8qq2t1a5du+R2u9XR0SFJ8ng8ysjIGJSAAAAgccT0jMXWrVsVCAQ0b9485eXlRZadO3cOVj4AAJBAYr4VAgAA0Bu+KwQAAFhDsQAAANZQLAAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgDcUCAABYQ7EAAADWUCwAAIA1FAsAAGANxQIAAFhDsQAAANZQLAAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgDcUCAABYM8zpAAOxp/2otWOVeadZOxaQrPhzAqAvXLEAAADWUCwAAIA1FAsAAGANxQIAAFgzoGLx7LPPKiUlRWvXrrUUBwAAJLJ+F4sjR47oj3/8o6ZMmWIzDwAASGD9Khbnzp3TypUr9fLLL2vkyJG2MwEAgATVr2Lh8/l0xx13qLS0tM9tQ6GQgsFg1AIAAJJTzB+Q9dprr6m5uVlHjhz5Xtv7/X5t2rQp5mAAACDxxHTFoq2tTY888oi2b9+uESNGfK99qqqqFAgEIktbW1u/ggIAgKEvpisWTU1N6uzs1A9/+MPIup6eHu3fv19/+MMfFAqFlJaWFrWPy+WSy+WykxYAAAxpMRWL+fPn66OPPopaV1FRoUmTJunxxx+/pFQAAICrS0zFwu12q6ioKGrdtddeq1GjRl2yHgAAXH345E0AAGDNgL82vaGhwUIMAACQDLhiAQAArKFYAAAAawZ8K6S/6o5/pEz3wHpNmXeanTAAAMAKrlgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGuGOXXiuyYWa1jKcKdODyAJ7Gk/au1YZd5p1o4FXM24YgEAAKyhWAAAAGsoFgAAwBqKBQAAsIZiAQAArIm5WHz22Wf6xS9+oVGjRikjI0PFxcV6//33ByMbAABIMDG93fTLL7/UnDlz9NOf/lR///vfdcMNN+jEiRMaOXLkYOUDAAAJJKZi8dxzzyk/P1+vvPJKZF1hYaH1UAAAIDHFdCvkzTff1IwZM/Tzn/9c2dnZ+sEPfqCXX375ivuEQiEFg8GoBQAAJKeYisW///1vbd26VTfffLP27NmjBx98UGvWrNG2bdt63cfv98vj8USW/Pz8AYcGAABDU4oxxnzfjdPT0zVjxgwdPHgwsm7NmjU6cuSIGhsbL7tPKBRSKBSK/B4MBpWfn695WsJHegMYED7SG4ifi6ZbDdqlQCCgzMzMXreL6YpFXl6ebrvttqh1t956q06dOtXrPi6XS5mZmVELAABITjEVizlz5qilpSVq3fHjx1VQUGA1FAAASEwxFYt169bp0KFD+u1vf6uTJ0+qtrZWf/rTn+Tz+QYrHwAASCAxFYuZM2eqrq5OO3bsUFFRkZ5++mnV1NRo5cqVg5UPAAAkkJg+x0KS7rzzTt15552DkQUAACQ4visEAABYQ7EAAADWxHwrBEBiSebPehhqeQBwxQIAAFhEsQAAANZQLAAAgDUUCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgDcUCAABYQ7EAAADWUCwAAIA1FAsAAGANxQIAAFhDsQAAANZQLAAAgDUUCwAAYA3FAgAAWDPM6QDAULCn/ai1Y5V5p1k7lg1DLQ+A5MYVCwAAYA3FAgAAWEOxAAAA1lAsAACANRQLAABgTUzFoqenRxs2bFBhYaEyMjI0YcIEPf300zLGDFY+AACQQGJ6u+lzzz2nrVu3atu2bZo8ebLef/99VVRUyOPxaM2aNYOVEQAAJIiYisXBgwe1ZMkS3XHHHZKkcePGaceOHTp8+PCghAMAAIklplshP/rRj1RfX6/jx49Lkj744AMdOHBAixYt6nWfUCikYDAYtQAAgOQU0xWLJ554QsFgUJMmTVJaWpp6enr0zDPPaOXKlb3u4/f7tWnTpgEHBQAAQ19MVyxef/11bd++XbW1tWpubta2bdv0u9/9Ttu2bet1n6qqKgUCgcjS1tY24NAAAGBoiumKxfr16/XEE09oxYoVkqTi4mJ98skn8vv9Ki8vv+w+LpdLLpdr4EkBAMCQF9MVi/Pnzys1NXqXtLQ0hcNhq6EAAEBiiumKxeLFi/XMM89o7Nixmjx5sv7xj39o8+bN+uUvfzlY+QAAQAKJqVi8+OKL2rBhgx566CF1dnbK6/Xq17/+tTZu3DhY+QAAQAKJqVi43W7V1NSopqZmkOIAAIBExneFAAAAaygWAADAmphuhQDJqsw7zekIAPphT/tRK8fh7wB7uGIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsIZiAQAArKFYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALBmWLxPaIyRJF1Ut2TifXYAQDIJng1bOc5F023lOMnsor4Zo2//He9NiulrC8s+/fRT5efnx/OUAADAkra2No0ZM6bX1+NeLMLhsNrb2+V2u5WSknLZbYLBoPLz89XW1qbMzMx4xrsqMd7xw1jHF+MdX4x3fMV7vI0xOnv2rLxer1JTe3+SIu63QlJTU6/YdP6/zMxMJmccMd7xw1jHF+MdX4x3fMVzvD0eT5/b8PAmAACwhmIBAACsGZLFwuVyqbq6Wi6Xy+koVwXGO34Y6/hivOOL8Y6voTrecX94EwAAJK8hecUCAAAkJooFAACwhmIBAACsoVgAAABrhlyx2LJli8aNG6cRI0aopKREhw8fdjpSUnrqqaeUkpIStUyaNMnpWElj//79Wrx4sbxer1JSUvTXv/416nVjjDZu3Ki8vDxlZGSotLRUJ06ccCZsEuhrvO+7775L5vvChQudCZvg/H6/Zs6cKbfbrezsbC1dulQtLS1R23z99dfy+XwaNWqUrrvuOi1btkxnzpxxKHFi+z7jPW/evEvm9wMPPOBQ4iFWLHbu3KnKykpVV1erublZU6dOVVlZmTo7O52OlpQmT56s06dPR5YDBw44HSlpdHV1aerUqdqyZctlX3/++ef1+9//Xi+99JLee+89XXvttSorK9PXX38d56TJoa/xlqSFCxdGzfcdO3bEMWHy2Ldvn3w+nw4dOqS9e/equ7tbCxYsUFdXV2SbdevW6a233tIbb7yhffv2qb29XXfffbeDqRPX9xlvSVq1alXU/H7++ecdSizJDCGzZs0yPp8v8ntPT4/xer3G7/c7mCo5VVdXm6lTpzod46ogydTV1UV+D4fDJjc317zwwguRdV999ZVxuVxmx44dDiRMLt8db2OMKS8vN0uWLHEkT7Lr7Ow0ksy+ffuMMd/M5eHDh5s33ngjss0///lPI8k0NjY6FTNpfHe8jTHmJz/5iXnkkUecC/UdQ+aKxYULF9TU1KTS0tLIutTUVJWWlqqxsdHBZMnrxIkT8nq9Gj9+vFauXKlTp045Hemq8PHHH6ujoyNqrns8HpWUlDDXB1FDQ4Oys7N1yy236MEHH9QXX3zhdKSkEAgEJElZWVmSpKamJnV3d0fN70mTJmns2LHMbwu+O97f2r59u0aPHq2ioiJVVVXp/PnzTsST5MCXkPXm888/V09Pj3JycqLW5+Tk6F//+pdDqZJXSUmJXn31Vd1yyy06ffq0Nm3apB//+Mc6duyY3G630/GSWkdHhyRddq5/+xrsWrhwoe6++24VFhaqtbVVTz75pBYtWqTGxkalpaU5HS9hhcNhrV27VnPmzFFRUZGkb+Z3enq6rr/++qhtmd8Dd7nxlqR7771XBQUF8nq9+vDDD/X444+rpaVFf/nLXxzJOWSKBeJr0aJFkZ+nTJmikpISFRQU6PXXX9f999/vYDLAvhUrVkR+Li4u1pQpUzRhwgQ1NDRo/vz5DiZLbD6fT8eOHeP5rDjpbbx/9atfRX4uLi5WXl6e5s+fr9bWVk2YMCHeMYfOw5ujR49WWlraJU8OnzlzRrm5uQ6lunpcf/31mjhxok6ePOl0lKT37Xxmrjtn/PjxGj16NPN9AFavXq23335b7777rsaMGRNZn5ubqwsXLuirr76K2p75PTC9jffllJSUSJJj83vIFIv09HRNnz5d9fX1kXXhcFj19fWaPXu2g8muDufOnVNra6vy8vKcjpL0CgsLlZubGzXXg8Gg3nvvPeZ6nHz66af64osvmO/9YIzR6tWrVVdXp3feeUeFhYVRr0+fPl3Dhw+Pmt8tLS06deoU87sf+hrvyzl69KgkOTa/h9StkMrKSpWXl2vGjBmaNWuWampq1NXVpYqKCqejJZ1HH31UixcvVkFBgdrb21VdXa20tDTdc889TkdLCufOnYv638LHH3+so0ePKisrS2PHjtXatWv1m9/8RjfffLMKCwu1YcMGeb1eLV261LnQCexK452VlaVNmzZp2bJlys3NVWtrqx577DHddNNNKisrczB1YvL5fKqtrdWuXbvkdrsjz014PB5lZGTI4/Ho/vvvV2VlpbKyspSZmamHH35Ys2fP1u233+5w+sTT13i3traqtrZWP/vZzzRq1Ch9+OGHWrdunebOnaspU6Y4E9rpt6V814svvmjGjh1r0tPTzaxZs8yhQ4ecjpSUli9fbvLy8kx6erq58cYbzfLly83JkyedjpU03n33XSPpkqW8vNwY881bTjds2GBycnKMy+Uy8+fPNy0tLc6GTmBXGu/z58+bBQsWmBtuuMEMHz7cFBQUmFWrVpmOjg6nYyeky42zJPPKK69Etvnvf/9rHnroITNy5EhzzTXXmLvuusucPn3audAJrK/xPnXqlJk7d67JysoyLpfL3HTTTWb9+vUmEAg4lpmvTQcAANYMmWcsAABA4qNYAAAAaygWAADAGooFAACwhmIBAACsoVgAAABrKBYAAMAaigUAALCGYgEAAKyhWAAAAGsoFgAAwBqKBQAAsOZ/ANk+n0mMmuEFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encode\n",
    "def one_hot_encode(index: int) -> np.array:\n",
    "    \"\"\"Return a one-hot encoded vector of the given index.\"\"\"\n",
    "    vector = np.zeros(len(characters))\n",
    "    vector[index] = 1\n",
    "    return vector\n",
    "\n",
    "x_encoded = np.array([one_hot_encode(i) for i in x])\n",
    "y_encoded = np.array([one_hot_encode(i) for i in y])\n",
    "\n",
    "print(x_encoded[:6])\n",
    "plt.imshow(x_encoded[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0000, training loss 3.325\n",
      "Epoch 0, Batch 0200, training loss 3.236\n",
      "Epoch 0, Batch 0400, training loss 3.11\n",
      "Epoch 0, Batch 0600, training loss 2.989\n",
      "Epoch 0, Batch 0800, training loss 2.848\n",
      "Epoch 0, Batch 1000, training loss 2.984\n",
      "Epoch 0, Batch 1200, training loss 2.713\n",
      "Epoch 0, Batch 1400, training loss 2.868\n",
      "Epoch 0, Batch 1600, training loss 2.725\n",
      "Epoch 0, Batch 1800, training loss 2.793\n",
      "Epoch 0, Batch 2000, training loss 2.79\n",
      "Epoch 0, Batch 2200, training loss 2.849\n",
      "Epoch 0, Batch 2400, training loss 2.697\n",
      "Epoch 0, Batch 2600, training loss 2.987\n",
      "Epoch 0, Batch 2800, training loss 2.767\n",
      "Epoch 0, Batch 3000, training loss 2.688\n",
      "Epoch 0, Batch 3200, training loss 2.6\n",
      "Epoch 0, Batch 3400, training loss 2.693\n",
      "Epoch 0, Batch 3600, training loss 3.018\n",
      "Epoch 0, Batch 3800, training loss 2.659\n",
      "Epoch 0, Batch 4000, training loss 2.747\n",
      "Epoch 0, Batch 4200, training loss 2.687\n",
      "Epoch 0, Batch 4400, training loss 2.862\n",
      "Epoch 0, Batch 4600, training loss 2.803\n",
      "Epoch 0, Batch 4800, training loss 2.639\n",
      "Epoch 0, Batch 5000, training loss 2.653\n",
      "Epoch 0, Batch 5200, training loss 2.726\n",
      "Epoch 0, Batch 5400, training loss 2.637\n",
      "Epoch 0, Batch 5600, training loss 2.569\n",
      "Epoch 0, Batch 5800, training loss 2.663\n",
      "Epoch 0, Batch 6000, training loss 2.499\n",
      "Epoch 0, Batch 6200, training loss 2.556\n",
      "Epoch 0, Batch 6400, training loss 2.745\n",
      "Epoch 0, Batch 6600, training loss 2.577\n",
      "Epoch 0, Batch 6800, training loss 2.957\n",
      "Epoch 0, Batch 7000, training loss 2.659\n",
      "Epoch 1, Batch 0000, training loss 2.611\n",
      "Epoch 1, Batch 0200, training loss 2.53\n",
      "Epoch 1, Batch 0400, training loss 2.748\n",
      "Epoch 1, Batch 0600, training loss 2.582\n",
      "Epoch 1, Batch 0800, training loss 2.685\n",
      "Epoch 1, Batch 1000, training loss 2.544\n",
      "Epoch 1, Batch 1200, training loss 2.551\n",
      "Epoch 1, Batch 1400, training loss 2.856\n",
      "Epoch 1, Batch 1600, training loss 2.364\n",
      "Epoch 1, Batch 1800, training loss 2.572\n",
      "Epoch 1, Batch 2000, training loss 2.738\n",
      "Epoch 1, Batch 2200, training loss 3.005\n",
      "Epoch 1, Batch 2400, training loss 2.35\n",
      "Epoch 1, Batch 2600, training loss 2.95\n",
      "Epoch 1, Batch 2800, training loss 2.457\n",
      "Epoch 1, Batch 3000, training loss 2.509\n",
      "Epoch 1, Batch 3200, training loss 2.28\n",
      "Epoch 1, Batch 3400, training loss 2.848\n",
      "Epoch 1, Batch 3600, training loss 2.708\n",
      "Epoch 1, Batch 3800, training loss 2.51\n",
      "Epoch 1, Batch 4000, training loss 2.373\n",
      "Epoch 1, Batch 4200, training loss 2.673\n",
      "Epoch 1, Batch 4400, training loss 2.539\n",
      "Epoch 1, Batch 4600, training loss 2.446\n",
      "Epoch 1, Batch 4800, training loss 2.633\n",
      "Epoch 1, Batch 5000, training loss 2.751\n",
      "Epoch 1, Batch 5200, training loss 2.61\n",
      "Epoch 1, Batch 5400, training loss 2.387\n",
      "Epoch 1, Batch 5600, training loss 2.262\n",
      "Epoch 1, Batch 5800, training loss 2.916\n",
      "Epoch 1, Batch 6000, training loss 2.309\n",
      "Epoch 1, Batch 6200, training loss 2.414\n",
      "Epoch 1, Batch 6400, training loss 2.621\n",
      "Epoch 1, Batch 6600, training loss 2.577\n",
      "Epoch 1, Batch 6800, training loss 2.254\n",
      "Epoch 1, Batch 7000, training loss 2.957\n",
      "Epoch 2, Batch 0000, training loss 2.676\n",
      "Epoch 2, Batch 0200, training loss 2.218\n",
      "Epoch 2, Batch 0400, training loss 2.565\n",
      "Epoch 2, Batch 0600, training loss 2.751\n",
      "Epoch 2, Batch 0800, training loss 2.582\n",
      "Epoch 2, Batch 1000, training loss 2.521\n",
      "Epoch 2, Batch 1200, training loss 2.505\n",
      "Epoch 2, Batch 1400, training loss 2.221\n",
      "Epoch 2, Batch 1600, training loss 2.622\n",
      "Epoch 2, Batch 1800, training loss 2.563\n",
      "Epoch 2, Batch 2000, training loss 2.564\n",
      "Epoch 2, Batch 2200, training loss 2.668\n",
      "Epoch 2, Batch 2400, training loss 2.806\n",
      "Epoch 2, Batch 2600, training loss 2.65\n",
      "Epoch 2, Batch 2800, training loss 2.533\n",
      "Epoch 2, Batch 3000, training loss 2.611\n",
      "Epoch 2, Batch 3200, training loss 2.614\n",
      "Epoch 2, Batch 3400, training loss 2.553\n",
      "Epoch 2, Batch 3600, training loss 2.405\n",
      "Epoch 2, Batch 3800, training loss 2.417\n",
      "Epoch 2, Batch 4000, training loss 2.327\n",
      "Epoch 2, Batch 4200, training loss 2.593\n",
      "Epoch 2, Batch 4400, training loss 2.535\n",
      "Epoch 2, Batch 4600, training loss 2.503\n",
      "Epoch 2, Batch 4800, training loss 2.329\n",
      "Epoch 2, Batch 5000, training loss 2.551\n",
      "Epoch 2, Batch 5200, training loss 2.929\n",
      "Epoch 2, Batch 5400, training loss 2.802\n",
      "Epoch 2, Batch 5600, training loss 2.813\n",
      "Epoch 2, Batch 5800, training loss 2.864\n",
      "Epoch 2, Batch 6000, training loss 2.39\n",
      "Epoch 2, Batch 6200, training loss 2.273\n",
      "Epoch 2, Batch 6400, training loss 2.457\n",
      "Epoch 2, Batch 6600, training loss 2.549\n",
      "Epoch 2, Batch 6800, training loss 2.561\n",
      "Epoch 2, Batch 7000, training loss 2.781\n",
      "Epoch 3, Batch 0000, training loss 2.564\n",
      "Epoch 3, Batch 0200, training loss 2.597\n",
      "Epoch 3, Batch 0400, training loss 2.356\n",
      "Epoch 3, Batch 0600, training loss 2.563\n",
      "Epoch 3, Batch 0800, training loss 2.369\n",
      "Epoch 3, Batch 1000, training loss 2.867\n",
      "Epoch 3, Batch 1200, training loss 2.698\n",
      "Epoch 3, Batch 1400, training loss 2.4\n",
      "Epoch 3, Batch 1600, training loss 2.452\n",
      "Epoch 3, Batch 1800, training loss 2.697\n",
      "Epoch 3, Batch 2000, training loss 2.64\n",
      "Epoch 3, Batch 2200, training loss 2.718\n",
      "Epoch 3, Batch 2400, training loss 2.612\n",
      "Epoch 3, Batch 2600, training loss 2.495\n",
      "Epoch 3, Batch 2800, training loss 2.76\n",
      "Epoch 3, Batch 3000, training loss 2.767\n",
      "Epoch 3, Batch 3200, training loss 2.495\n",
      "Epoch 3, Batch 3400, training loss 2.247\n",
      "Epoch 3, Batch 3600, training loss 2.783\n",
      "Epoch 3, Batch 3800, training loss 2.509\n",
      "Epoch 3, Batch 4000, training loss 2.602\n",
      "Epoch 3, Batch 4200, training loss 2.474\n",
      "Epoch 3, Batch 4400, training loss 2.625\n",
      "Epoch 3, Batch 4600, training loss 2.068\n",
      "Epoch 3, Batch 4800, training loss 2.81\n",
      "Epoch 3, Batch 5000, training loss 2.514\n",
      "Epoch 3, Batch 5200, training loss 2.512\n",
      "Epoch 3, Batch 5400, training loss 2.788\n",
      "Epoch 3, Batch 5600, training loss 2.873\n",
      "Epoch 3, Batch 5800, training loss 2.62\n",
      "Epoch 3, Batch 6000, training loss 2.403\n",
      "Epoch 3, Batch 6200, training loss 2.482\n",
      "Epoch 3, Batch 6400, training loss 2.591\n",
      "Epoch 3, Batch 6600, training loss 2.602\n",
      "Epoch 3, Batch 6800, training loss 2.429\n",
      "Epoch 3, Batch 7000, training loss 2.474\n",
      "Epoch 4, Batch 0000, training loss 2.462\n",
      "Epoch 4, Batch 0200, training loss 2.252\n",
      "Epoch 4, Batch 0400, training loss 2.653\n",
      "Epoch 4, Batch 0600, training loss 2.589\n",
      "Epoch 4, Batch 0800, training loss 2.618\n",
      "Epoch 4, Batch 1000, training loss 2.381\n",
      "Epoch 4, Batch 1200, training loss 2.524\n",
      "Epoch 4, Batch 1400, training loss 2.68\n",
      "Epoch 4, Batch 1600, training loss 2.424\n",
      "Epoch 4, Batch 1800, training loss 2.479\n",
      "Epoch 4, Batch 2000, training loss 2.185\n",
      "Epoch 4, Batch 2200, training loss 2.036\n",
      "Epoch 4, Batch 2400, training loss 2.858\n",
      "Epoch 4, Batch 2600, training loss 2.436\n",
      "Epoch 4, Batch 2800, training loss 2.423\n",
      "Epoch 4, Batch 3000, training loss 2.606\n",
      "Epoch 4, Batch 3200, training loss 2.406\n",
      "Epoch 4, Batch 3400, training loss 2.344\n",
      "Epoch 4, Batch 3600, training loss 2.544\n",
      "Epoch 4, Batch 3800, training loss 2.345\n",
      "Epoch 4, Batch 4000, training loss 2.5\n",
      "Epoch 4, Batch 4200, training loss 2.442\n",
      "Epoch 4, Batch 4400, training loss 2.488\n",
      "Epoch 4, Batch 4600, training loss 2.672\n",
      "Epoch 4, Batch 4800, training loss 2.344\n",
      "Epoch 4, Batch 5000, training loss 2.229\n",
      "Epoch 4, Batch 5200, training loss 2.336\n",
      "Epoch 4, Batch 5400, training loss 2.625\n",
      "Epoch 4, Batch 5600, training loss 2.312\n",
      "Epoch 4, Batch 5800, training loss 2.248\n",
      "Epoch 4, Batch 6000, training loss 2.228\n",
      "Epoch 4, Batch 6200, training loss 2.528\n",
      "Epoch 4, Batch 6400, training loss 2.115\n",
      "Epoch 4, Batch 6600, training loss 2.071\n",
      "Epoch 4, Batch 6800, training loss 2.82\n",
      "Epoch 4, Batch 7000, training loss 2.842\n",
      "Epoch 5, Batch 0000, training loss 2.598\n",
      "Epoch 5, Batch 0200, training loss 2.366\n",
      "Epoch 5, Batch 0400, training loss 2.241\n",
      "Epoch 5, Batch 0600, training loss 2.895\n",
      "Epoch 5, Batch 0800, training loss 2.369\n",
      "Epoch 5, Batch 1000, training loss 2.857\n",
      "Epoch 5, Batch 1200, training loss 2.538\n",
      "Epoch 5, Batch 1400, training loss 2.579\n",
      "Epoch 5, Batch 1600, training loss 2.555\n",
      "Epoch 5, Batch 1800, training loss 2.46\n",
      "Epoch 5, Batch 2000, training loss 2.394\n",
      "Epoch 5, Batch 2200, training loss 2.461\n",
      "Epoch 5, Batch 2400, training loss 2.189\n",
      "Epoch 5, Batch 2600, training loss 2.589\n",
      "Epoch 5, Batch 2800, training loss 2.176\n",
      "Epoch 5, Batch 3000, training loss 2.458\n",
      "Epoch 5, Batch 3200, training loss 2.248\n",
      "Epoch 5, Batch 3400, training loss 2.485\n",
      "Epoch 5, Batch 3600, training loss 2.414\n",
      "Epoch 5, Batch 3800, training loss 2.29\n",
      "Epoch 5, Batch 4000, training loss 2.558\n",
      "Epoch 5, Batch 4200, training loss 2.399\n",
      "Epoch 5, Batch 4400, training loss 2.234\n",
      "Epoch 5, Batch 4600, training loss 2.452\n",
      "Epoch 5, Batch 4800, training loss 2.552\n",
      "Epoch 5, Batch 5000, training loss 2.386\n",
      "Epoch 5, Batch 5200, training loss 2.413\n",
      "Epoch 5, Batch 5400, training loss 2.36\n",
      "Epoch 5, Batch 5600, training loss 2.366\n",
      "Epoch 5, Batch 5800, training loss 2.279\n",
      "Epoch 5, Batch 6000, training loss 2.497\n",
      "Epoch 5, Batch 6200, training loss 2.472\n",
      "Epoch 5, Batch 6400, training loss 2.419\n",
      "Epoch 5, Batch 6600, training loss 2.735\n",
      "Epoch 5, Batch 6800, training loss 2.488\n",
      "Epoch 5, Batch 7000, training loss 2.386\n",
      "Epoch 6, Batch 0000, training loss 2.514\n",
      "Epoch 6, Batch 0200, training loss 2.76\n",
      "Epoch 6, Batch 0400, training loss 2.337\n",
      "Epoch 6, Batch 0600, training loss 2.7\n",
      "Epoch 6, Batch 0800, training loss 2.382\n",
      "Epoch 6, Batch 1000, training loss 2.652\n",
      "Epoch 6, Batch 1200, training loss 2.365\n",
      "Epoch 6, Batch 1400, training loss 2.401\n",
      "Epoch 6, Batch 1600, training loss 2.346\n",
      "Epoch 6, Batch 1800, training loss 2.342\n",
      "Epoch 6, Batch 2000, training loss 2.424\n",
      "Epoch 6, Batch 2200, training loss 2.787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 49\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, targets)\n\u001b[1;32m     51\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Forward pass through the network.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m        Tensor after passing through the layers and activations.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"A Multilayer Perceptron (MLP) model in PyTorch with one hidden layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(27, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 27),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor after passing through the layers and activations.\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "x_encoded = torch.tensor(x_encoded, dtype=torch.float).to(device)\n",
    "y_encoded = torch.tensor(y_encoded, dtype=torch.float).to(device)\n",
    "\n",
    "dataset = TensorDataset(x_encoded, y_encoded)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = MLP()\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 200 == 0:\n",
    "            losses.append(loss.item())\n",
    "            print(f\"Epoch {epoch}, Batch {i:04}, training loss {round(loss.item(), 3)}\")\n",
    "print(f\"Epoch {epoch}, Batch {i:04}, training loss {round(loss.item(), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses over time\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
