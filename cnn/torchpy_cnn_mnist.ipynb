{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set  : X-(56000, 1, 28, 28), y-(56000,)\n",
      "Validation set: X-(7000, 1, 28, 28), y-(7000,)\n",
      "Test set      : X-(7000, 1, 28, 28), y-(7000,)\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "DIMENSIONS = (28, 28)\n",
    "CHANNELS = 1  # grayscale\n",
    "INPUT_SIZE = DIMENSIONS[0] * DIMENSIONS[1]\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "def transform_data(\n",
    "        is_convolutional: bool,\n",
    "        x: pd.DataFrame,\n",
    "        y: pd.Series | None = None) -> tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Transforms the data. Returns a tuple of (x, y) where x is a tensor of the features and y is a\n",
    "    tensor of the labels.\n",
    "\n",
    "    Args:\n",
    "        is_convolutional: A boolean indicating whether the model is convolutional.\n",
    "        x: A dataframe of the features.\n",
    "        y: A series of the labels.\n",
    "    \"\"\"\n",
    "    x = x.to_numpy()\n",
    "    # Normalize the tensor\n",
    "    x_min = x.min()\n",
    "    x_max = x.max()\n",
    "    x = (x - x_min) / (x_max - x_min)\n",
    "    assert x.min() == 0\n",
    "    assert x.max() == 1\n",
    "    if is_convolutional:\n",
    "        # Reshape data to have channel dimension\n",
    "        # MNIST images are 28x28, so we reshape them to [batch_size, 1, 28, 28]\n",
    "        x = x.reshape(-1, CHANNELS, DIMENSIONS[0], DIMENSIONS[1])\n",
    "    if y is not None:\n",
    "        y = y.to_numpy().astype(int)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_data():  # noqa\n",
    "    \"\"\"Function is required by and called from `model_pipeline()`.\"\"\"\n",
    "    x, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto')\n",
    "    x, y = transform_data(is_convolutional=True, x=x, y=y)\n",
    "    # 80% train; 10% validation; 10% test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=SEED)\n",
    "    x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=SEED)  # noqa\n",
    "    print(f\"Training set  : X-{x_train.shape}, y-{y_train.shape}\")\n",
    "    print(f\"Validation set: X-{x_val.shape}, y-{y_val.shape}\")\n",
    "    print(f\"Test set      : X-{x_test.shape}, y-{y_test.shape}\")\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0000, training loss 2.322, validation loss 2.319\n",
      "Epoch 0, Batch 0800, training loss 0.099, validation loss 0.239\n"
     ]
    }
   ],
   "source": [
    "from torchpy import net\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "shuffle_index = rng.permutation(x_train.shape[0])\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "import torchpy.architectures as arch\n",
    "model = arch.CNNMnist()\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_function = net.CrossEntropyLoss()\n",
    "optimizer = net.SGD(learning_rate=learning_rate)\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for batch, i in enumerate(range(0, x_train.shape[0], batch_size)):\n",
    "        x_batch = x_train[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        with net.training_mode():\n",
    "            # Forward pass\n",
    "            logits = model(x_batch)\n",
    "            loss = loss_function(logits, y_batch)\n",
    "            # Backward pass\n",
    "            loss_grad = loss_function.backward()\n",
    "            model.backward(loss_grad)\n",
    "            model.step(optimizer)\n",
    "        if batch % 800 == 0:\n",
    "            training_losses.append(loss)\n",
    "            # calculate validation loss\n",
    "            val_loss = loss_function(logits=model(x_val), targets=y_val)\n",
    "            validation_losses.append(val_loss)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Batch {batch:04}, training loss {round(loss, 3)}, \",\n",
    "                f\"validation loss {round(val_loss, 3)} - {time.time() - start:.2f}s\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(f\"Training took {round(end - start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses over time\n",
    "plt.plot(training_losses, label=\"Training loss\")\n",
    "plt.plot(validation_losses, label=\"Validation loss\")\n",
    "plt.ylim(0, 3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(x_test)\n",
    "loss = loss_function(logits, y_test)\n",
    "print(f\"Test loss: {loss}\")\n",
    "# get index of highest probability in each row\n",
    "predicted = np.argmax(logits, axis=1)\n",
    "accuracy = (predicted == y_test).sum() / y_test.size\n",
    "print(f\"Test accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predicted)\n",
    "# remove the diagonal values (correct predictions) for better visualization\n",
    "np.fill_diagonal(cm, 0)\n",
    "fig = plt.figure(figsize=(OUTPUT_SIZE, OUTPUT_SIZE))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Count of Misclassified Samples by Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
